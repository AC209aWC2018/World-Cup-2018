{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sofifa.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenced from https://realpython.com/python-web-scraping-practical-introduction/\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to scrape the content at 'url' by making a HTTP GET request. \n",
    "    If the content-type of the response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    from contextlib import closing\n",
    "    \n",
    "    try:\n",
    "        with closing(requests.get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "    except AssertionError as error:\n",
    "        print(error)\n",
    "        print('Error in scraping of url')\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if response is some kind of HTML/XML\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200\n",
    "           and content_type is not None\n",
    "           and content_type.find('html') > -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the information on the website is categorized by FIFA update dates. For each FIFA update date, we have different national teams and national players with different statistics. We note that there does not seem to be a strict order to the update dates. Some months have several updates, while others have only 1 update. We have all the data starting from August 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_href():\n",
    "    url = 'https://sofifa.com/'\n",
    "    html = BeautifulSoup(simple_get(url), 'html.parser')\n",
    "    \n",
    "    dates_href = {}\n",
    "    \n",
    "    # for each month\n",
    "    for m in html.find_all('div', attrs={'class': 'card-title h5'})[:-8]: # note the last 8 items are not dates\n",
    "        month = m.get_text()\n",
    "        \n",
    "        # for each day of the month\n",
    "        for d in m.find_next('div').find_all('a'):\n",
    "            day = d.get_text()\n",
    "            date = day+' '+month\n",
    "            href = d.get('href')\n",
    "            \n",
    "            dates_href[date] = href\n",
    "    \n",
    "    return dates_href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_href = get_date_href()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the href attributes for each national team\n",
    "\n",
    "def get_nation_href(date_url):\n",
    "    \"\"\"Given a date url corresponding to a specific update of FIFA 18, returns a dictionary \n",
    "    containing the teams and their hrefs\"\"\"\n",
    "    url = 'https://sofifa.com/teams/national'+date_url   \n",
    "    html = BeautifulSoup(simple_get(url), 'html.parser')\n",
    "\n",
    "    teams_href = {}\n",
    "\n",
    "    for link in html.find_all('a', attrs={'href': re.compile(\"^/team/.+\")}):\n",
    "        if link.get_text() not in teams_href:\n",
    "            teams_href[link.get_text()] = link.get('href')\n",
    "            \n",
    "    return teams_href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teams_href = get_nation_href('?v=WC18&e=159126&set=true') # corresponds to FIFA WC18 Expansion Jun 16\n",
    "len(teams_href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that at different dates, there may be different teams represented in the game. As such, we loop through all the possible dates to find the full list of all teams that have ever been represented in the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_teams_href = {}\n",
    "\n",
    "for d, dhref in dates_href.items():    \n",
    "    teams_href = get_nation_href(dhref)\n",
    "    # merges dictionaries\n",
    "    full_teams_href = {**teams_href, **full_teams_href}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_teams_href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the urls of all 62 national teams that has ever appeared in the game. We note that the current urls link to the most recent ratings of the teams. We can find the urls of all players in the national teams from this page as well. We now wish to collect team specific data for all these teams across all the different dates. \n",
    "\n",
    "However, we note that we should not collecting team data for all of the possible dates that we have collected previously. At each version update, not all the teams are in the game. As such, we should be collecting historical team data for teams that appear at each version of the game. Thankfully, we can access the specific history of each team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://sofifa.com'\n",
    "team_ver_href = {}\n",
    "for t, thref in full_teams_href.items():\n",
    "    # create a new session\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(5)\n",
    "    driver.get(url+thref)\n",
    "    \n",
    "    html_ver = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    html_ver_dat = html_ver.select('select > option')[1:]\n",
    "    ver_href = [i.get('value') for i in html_ver_dat]\n",
    "    date = [i.get_text() for i in html_ver_dat]\n",
    "    \n",
    "    # get number of versions\n",
    "    n = len(ver_href)\n",
    "    \n",
    "    for i, v in enumerate(date):\n",
    "        team_ver_href[(t, v)] = ver_href[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then scrape the data for each version of every team that has ever appeared in FIFA. We scrape some of the team and individual player statistics that we think will be important in our model analysis together at the same time. We will then subsequently clean up the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 20594/20594 [6:01:53<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "team_stats = {}\n",
    "for tdate, vhref in tqdm(team_ver_href.items()):\n",
    "        url_team = 'https://sofifa.com'+vhref\n",
    "        html_team = BeautifulSoup(simple_get(url_team), 'lxml')\n",
    "        \n",
    "        span = html_team.find('div', attrs={'class': 'card-body stats'}).find_all('span')\n",
    "        span_label = (html_team\n",
    "                      .find('div', attrs={'class': 'card-body stats'})\n",
    "                      .find_all('span', attrs={'class': re.compile(\"label.+\")}))\n",
    "        \n",
    "        # get basic statistics\n",
    "        overall = float(span_label[0].get_text())\n",
    "        attack = float(span_label[1].get_text())\n",
    "        midfield = float(span_label[2].get_text())\n",
    "        defence = float(span_label[3].get_text())\n",
    "\n",
    "        prestige = float(html_team.find(string='International Prestige').find_next('span').get_text())\n",
    "        start_age = float(html_team.find(string='Starting XI Average Age').next_element)\n",
    "        full_age = float(html_team.find(string='Whole Team Average Age').next_element)\n",
    "        \n",
    "        # get other stats lumped together\n",
    "        stats = [i.get_text() for i in html_team.select('span[class=\"float-right\"] > span[class=\"label\"]')]\n",
    "        \n",
    "        # get individual player stats lumped together\n",
    "        name = [i.get_text() for i in html_team.select('td > div > a ~ a')]\n",
    "        player_overall = [i.get_text() for i in html_team.select('div[class=\"col-digit col-oa\"] > span')]\n",
    "        potential = [i.get_text() for i in html_team.select('div[class=\"col-digit col-pt\"] > span')]\n",
    "        value = [i.get_text() for i in html_team.select('div[class=\"col-digit col-vl\"]')]\n",
    "        wage = [i.get_text() for i in html_team.select('div[class=\"col-digit col-wg\"]')]\n",
    "        \n",
    "        team_stats[tdate] = {\n",
    "            'overall': overall,\n",
    "            'attack': attack,\n",
    "            'midfield': midfield,\n",
    "            'defence': defence,\n",
    "            'prestige': prestige,\n",
    "            'start_age': start_age,\n",
    "            'full_age': full_age,\n",
    "            'ext_stats': stats,\n",
    "            'name': name,\n",
    "            'player_overall': player_overall,\n",
    "            'potential': potential,\n",
    "            'value': value,\n",
    "            'wage': wage\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sofifa_data = pd.DataFrame(team_stats).T.reset_index()\n",
    "sofifa_data.rename({'level_0': 'team', 'level_1': 'date'}, axis=1, inplace=True)\n",
    "\n",
    "# save to csv\n",
    "sofifa_data.to_csv('data/sofifa_data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ac209a]",
   "language": "python",
   "name": "conda-env-ac209a-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
