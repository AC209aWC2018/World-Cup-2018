{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Baseline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with Feature Engineered Features\n",
    "\n",
    "To begin, we naively used all the features we have feature engineered. Similar to before, we fitted different classification models on our data and select the one with the best validation accuracy.\n",
    "\n",
    "\n",
    "We ran Logistic Regression, LDA, QDA, Random Forest, and XGBoost and got the following results: \n",
    "![Model scores](img/model_scores.png)\n",
    "\n",
    "We choose the final model to be the one with the highest validation score, which is LDA in this case.\n",
    "\n",
    "Similar to before, we used three different approaches to predict the test set. For the WC Playoff Model approach, we continue to use logistic regression. \n",
    "\n",
    "![LDA scores](img/lda_scores.png)\n",
    "\n",
    "It seems that our model with all the features is a decent improvement to the baseline model for the test set in all three approaches. However, the lower train and validation scores in general is a bit concerning. Like before, random forest seems to perform similarly to the best model, so we will utilize its feature importance again.\n",
    "\n",
    "![Random Forest features](img/RF_features.png)\n",
    "\n",
    "\n",
    "The ratings of the team are still pretty important features. Many of the features we have engineered seem to be decent features as well, which should not be surprising from the EDA. Besides 3 of them, many of the momentum features are near the bottom, and this might be due to the fact that these momentum features are obviously highly correlated with each other, and this may be causing the lower train and validation scores. Many of the features in our feature are highly correlated; we would want to deal with this multicollinearity somehow to see if we can achieve better results.\n",
    "\n",
    "### Stacking\n",
    "\n",
    "We saw that our models in general performed similarly in terms of train/validation. Why not try stacking the models together? We will stack the logistic regression, LDA, QDA, Random Forest, and XGBoost models.\n",
    "\n",
    "We got the following results: <br>\n",
    "Stacking Model Train Score: 0.5662491760052736 <br>\n",
    "Stacking Model Validation Score: 0.5368421052631579 <br>\n",
    "Stacking Model Test Score at 90 mins: 0.609375 <br>\n",
    "Stacking Model Test Score at end (Softmax): 0.625 <br>\n",
    "Stacking Model Test Score at end (WC Playoff Model): 0.640625\n",
    "\n",
    "It seems that stacking in this case did not really help that much. The test accuarcies are still decent though, higher than those of the baseline model.\n",
    "\n",
    "As mentioned before, we are a bit concerned about the fact that our features are highly correlated, which potentially affects our predictions.\n",
    "\n",
    "### Principal Component Regression\n",
    "\n",
    "Principal Component Analysis (PCA) is a way to reduce the dimensionality of dataset by summarizing the variation in our data into a set of new predictors called principal components. These principal components are linear combinations of our original predictors. By selecting the top few principal components, we are projecting our dataset into the space defined by these components. This means that we are projecting our dataset onto a space of smaller dimensionality. Importantly, this will help us reduce the multicollinearity that might be affecting our predictions. Each principal component is constructed such that they are orthogonal with each other. Nonetheless, we understand that this will limit the interpretability of our coefficients.\n",
    "\n",
    "Principal Component Regression (PCR) is basically using the new predictors of reduced dimensionality in a regression problem. In this problem, we inputed the new principal components into our standard logistic regression. In order to decide the number of principal components to keep, we cross-validated the number of principal components that gives us the best validation accuracy. For the WC Playoffs Model, we will also use PCA to find the number of components that maximizes the train accuracy and fit a PCR model to the World Cup playoffs data. \n",
    "\n",
    "![PCR Plot](img/pcr_plot.png)\n",
    "\n",
    "\n",
    "We got the following results: <br>\n",
    "Best Validation Accuracy Number of Components: 3 <br>\n",
    "PCR (Best) Train Score: 0.5042847725774555 <br>\n",
    "PCR (Best) Validation Score: 0.5473684210526316 <br>\n",
    "PCR (Best) Test Score at 90 mins: 0.5625 <br>\n",
    "PCR (Best) Test Score at end (Softmax): 0.578125 <br>\n",
    "PCR (Best) Test Score at end (WC Playoffs Model): 0.609375\n",
    "\n",
    "In this case, PCR was not a good option. The test accuracy decreased quite significantly. We may need to take into the account the outcome as well when we reduce the dimension of our feature set.\n",
    "\n",
    "### Partial Least Squares Regression\n",
    "\n",
    "Another way that can help us deal with the issues of multicollinearity in our dataset is Partial Least Squares Regression (PLSR). Similar to PCA, PLSR involves projecting the predictors onto orthogonal components. However, the PLSR components are constructed such that they not only approximate the predictors, but are also well correlated with the response. As such, we assume that both the predictors and the response are functions of (reduced) principal components. In this problem, our response variable is a multi-class categorical variable. As such, we could use the PLS2 algorithm which simultaneously decomposes on the multi-class variable directly. We could also use the PLS1 algorithm on each category in our response variable separately. \n",
    "\n",
    "To demonstrate how the Partial Least Squares algorithms works, we have included a pseudo-code for PLS1:\n",
    "\n",
    "Set $X_0 = X$, and $y_0 = y$\n",
    "\n",
    "for $h = 1, 2, ... r$ do (where r is the dimension of the predictors)\n",
    "\n",
    "$\\hspace{1cm} \\mathbf{w_h} = \\mathbf{X^T_{h-1}y_{h-1}/y^T_{h-1}y_{h-1}}$ (regress predictors $x_j$ on response $y$)\n",
    "\n",
    "$\\hspace{1cm} ||\\mathbf{w_h}|| = 1$ (normalize)\n",
    "\n",
    "$\\hspace{1cm} \\mathbf{z_h} = \\mathbf{X_{h-1}w_h/w^T_hw_h}$ (regress predictors $x_j$ on weights $w_h$)\n",
    "\n",
    "$\\hspace{1cm} \\mathbf{p_h} = \\mathbf{X^T_{h-1}z_h/z^T_hz_h}$ (regress predictors $x_j$ on components $z_h$)\n",
    "\n",
    "$\\hspace{1cm} \\mathbf{X_h} = \\mathbf{X_{h-1} - z_hp^T_h}$ (deflate $X_{h-1}$)\n",
    "\n",
    "$\\hspace{1cm} d_h = \\mathbf{y^T_hz_h/z^T_hz_h}$ (regress response $y_h$ onto components $z_h$)\n",
    "\n",
    "$\\hspace{1cm} \\mathbf{y_h} = \\mathbf{y_{h-1} - d_hz_h}$ (deflate $y_{h-1}$)\n",
    "\n",
    "end for\n",
    "\n",
    "The PLS2 algorithm is just an extension for a response variable with more than two outcomes.\n",
    "\n",
    "Importantly, PLS selects components which gives us the greatest reduction in the covariance of our predictors and response. If we represent our components as $\\mathbf{z}$, and the response as $\\mathbf{y}$, it can be shown that PLSR optimizes with respect to the weights $\\mathbf{w}$\n",
    "\n",
    "$$ \\arg\\max_{\\mathbf{w}} cor^2(\\mathbf{y, z})var(\\mathbf{y})var(\\mathbf{z})$$\n",
    "\n",
    "It can thus be seen that PLSR tries to maximize the correlation between the components and the response whilst trying to maximize the variance captured by the components. \n",
    "\n",
    "This not only allows us to resolve the multicollinearity issues through the creation of orthogonal components just as in PCR, but also allows us to create components that are correlated with the response. It is likely that this might perform even better than PCR.\n",
    "\n",
    "\n",
    "We got the following results for PLS1-DA: <br>\n",
    "Best Validation Accuracy Number of Components: 16 <br>\n",
    "PLS1-DA (Best) Train Score: 0.5227422544495716<br>\n",
    "PLS1-DA (Best) Validation Score: 0.5552631578947368<br>\n",
    "\n",
    "\n",
    "![PLS 1](img/pls-da1.png)\n",
    "\n",
    "We got the following results for PLS2-DA: <br>\n",
    "Best Validation Accuracy Number of Components: 9 <br>\n",
    "PLS2-DA (Best) Train Score: 0.5181278839815425 <br>\n",
    "PLS2-DA (Best) Validation Score: 0.5526315789473685 <br>\n",
    "\n",
    "![PLS 1](img/pls-da2.png)\n",
    "\n",
    "For the WC Playoffs Model, we will also fit the PLS model with number of components that maximizes the train accuracy. In the case of binary labels, PLS1-DA and PLS2-DA are the same, so we only have one result for the WC Playoffs Model.\n",
    "\n",
    "\n",
    "PLS1-DA (Best) Test Score at 90 mins: 0.640625 <br>\n",
    "PLS1-DA (Best) Test Score at end (Softmax): 0.6875 <br>\n",
    "PLS2-DA (Best) Test Score at 90 mins: 0.59375<br>\n",
    "PLS2-DA (Best) Test Score at end (Softmax): 0.625<br>\n",
    "PLS1-DA/PLS2-DA (Best) Test Score at end (WC Playoffs Model): 0.65625<br>\n",
    "\n",
    "Compared to the full model, we do not see much change in the PLS1-DA model on the test set while we see a decrease in performance of PLS2-DA on the test set, although PLS1-DA does perform better in the second approach than the full model does; thus PLS1-DA is the best model we have seen so far! What's more important is that the validation scores for PLS1-DA were higher than those in the full model. It is likely that multicollinearity was causing the decrease in our validation scores.\n",
    "\n",
    "Now, suppose we don't actually know anything about the test set. Which model would we have actually chosen? Like before, we can only look at the validation scores.\n",
    "\n",
    "![Final models](img/final_models.png)\n",
    "\n",
    "Based on validation scores, we would have chosen PLS1-DA, the best model!\n",
    "\n",
    "Let's see the confusion matrices for our best model, PLS1-DA.\n",
    "\n",
    "![Confusion Matrix](img/conf.png)\n",
    "\n",
    "The model is actually predicting some draws now for the training set and test set, although still very little. As mentioned before, it is just very hard to predict draws, but this is definitely an improvement compared to the baseline model.\n",
    "\n",
    "Compared to the baseline model, it seems that the model is better in predicting when \"home\" team loses in the test set in all three approaches. This might be due to some bias of how we feature engineer. Since none of us really were experts in soccer, we chose features that we observed from the World Cup alone, so these features might be biased toward the test set.\n",
    "\n",
    "The Softmax approach had higher home loss and home win accuracies than the WC Playoff Model approach; this might be due to the small training set for the playoff matches, so we could just be overfitting to the traing set. As mentioned in the baseline model, we cannot really compare the confusion matrices of the three approaches to the test set, as the true labels between the 90 minutes approach are different from those than the true labels in the other two approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
