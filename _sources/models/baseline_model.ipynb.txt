{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first build some basic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from predict_test_data import predict_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_team.csv')\n",
    "test = pd.read_csv('../data/test_team.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['home_win', 'attack_diff', 'bup_dribbling_diff', 'bup_passing_diff',\n",
       "       'bup_speed_diff', 'cc_crossing_diff', 'cc_passing_diff',\n",
       "       'cc_shooting_diff', 'd_aggresion_diff', 'd_pressure_diff',\n",
       "       'd_width_diff', 'defence_diff', 'full_age_diff',\n",
       "       'goalkeeeper_overall_diff', 'growth_diff', 'midfield_diff',\n",
       "       'overall_diff', 'prestige_diff', 'start_age_diff',\n",
       "       'value_euros_millions_diff', 'wage_euros_thousands_diff',\n",
       "       'attack_home_defence_away_diff', 'attack_away_defence_home_diff',\n",
       "       'cur_year_avg_weighted_diff', 'cur_year_avg_diff',\n",
       "       'last_year_avg_weighted_diff', 'last_year_avg_diff',\n",
       "       'previous_points_diff', 'rank_change_diff', 'rank_diff',\n",
       "       'three_year_ago_avg_diff', 'three_year_ago_weighted_diff',\n",
       "       'total_points_diff', 'two_year_ago_avg_diff',\n",
       "       'two_year_ago_weighted_diff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our most basic model would be to just predict the majority class every time. In this case, `home_win` = 1 is the majority class. What are is the training accuracy from just doing this \"prediction\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44168962350780533"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['home_win'].value_counts()[1] / len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty decent when we have 3 classes. What about the test accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.421875"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test['home_win'], np.ones(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still decent. Any model we build should be better than this test accuracy of just guessing.\n",
    "\n",
    "Our baseline model will be pretty simple. We will utilize the differences in FIFA rankings, offense ratings, defense ratings, midfield ratings, and overall ratings. We will make a train and validation set out of the original train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['home_win', 'attack_diff', 'defence_diff', 'midfield_diff', 'overall_diff']]\n",
    "test = test[['home_win', 'attack_diff', 'defence_diff', 'midfield_diff', 'overall_diff', 'Group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_train, X_validation = train_test_split(train, test_size = 0.2)\n",
    "y_train = X_train['home_win'].ravel()\n",
    "X_train = X_train.drop(['home_win'], axis = 1)\n",
    "y_validation = X_validation['home_win'].ravel()\n",
    "X_validation = X_validation.drop(['home_win'], axis = 1)\n",
    "y_test = test['home_win'].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try out logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegressionCV(solver = 'lbfgs', max_iter = 5000, cv = 5, multi_class='multinomial').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Score: 0.5040183696900115\n",
      "Logistic Regression Validation Score: 0.5596330275229358\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Train Score: {}\".format(lr_model.score(X_train, y_train)))\n",
    "print(\"Logistic Regression Validation Score: {}\".format(lr_model.score(X_validation, y_validation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also try out Linear Discriminant Analysis. However, we need to first check whether the variances across the three outcomes are equal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attack_diff</th>\n",
       "      <th>defence_diff</th>\n",
       "      <th>midfield_diff</th>\n",
       "      <th>overall_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_win</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.085986</td>\n",
       "      <td>38.149920</td>\n",
       "      <td>37.043922</td>\n",
       "      <td>31.591642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.649887</td>\n",
       "      <td>40.311426</td>\n",
       "      <td>38.643789</td>\n",
       "      <td>34.945340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.844219</td>\n",
       "      <td>46.110013</td>\n",
       "      <td>40.277586</td>\n",
       "      <td>37.455115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          attack_diff  defence_diff  midfield_diff  overall_diff\n",
       "home_win                                                        \n",
       "0           41.085986     38.149920      37.043922     31.591642\n",
       "1           47.649887     40.311426      38.643789     34.945340\n",
       "2           45.844219     46.110013      40.277586     37.455115"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('home_win').var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, they are actually quite similar! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LinearDiscriminantAnalysis().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Train Score: 0.4971297359357061\n",
      "LDA Validation Score: 0.5504587155963303\n"
     ]
    }
   ],
   "source": [
    "print(\"LDA Train Score: {}\".format(lda_model.score(X_train, y_train)))\n",
    "print(\"LDA Validation Score: {}\".format(lda_model.score(X_validation, y_validation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also try out Quadratic Discriminant Analysis, which should perform similarly to LDA in this case due to the almost equal variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_model = QuadraticDiscriminantAnalysis().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA Train Score: 0.5086107921928817\n",
      "QDA Validation Score: 0.5412844036697247\n"
     ]
    }
   ],
   "source": [
    "print(\"QDA Train Score: {}\".format(qda_model.score(X_train, y_train)))\n",
    "print(\"QDA Validation Score: {}\".format(qda_model.score(X_validation, y_validation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also try out Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(min_samples_leaf = 20, n_estimators=100).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Score: 0.5327210103329506\n",
      "Random Forest Validation Score 0.555045871559633\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Train Score: {}\".format(rf_model.score(X_train, y_train)))\n",
    "print(\"Random Forest Validation Score {}\".format(rf_model.score(X_validation, y_validation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how these models perform on the actual World Cup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Score: 0.609375\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Test Score: {}\".\n",
    "                format(accuracy_score(test['home_win'], predict_test_data(test, X_train.columns, lr_model))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Test Score: 0.59375\n"
     ]
    }
   ],
   "source": [
    "print(\"LDA Test Score: {}\".format(\n",
    "    accuracy_score(test['home_win'], predict_test_data(test, X_train.columns, lda_model))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA Test Score: 0.546875\n"
     ]
    }
   ],
   "source": [
    "print(\"QDA Test Score: {}\".format(\n",
    "    accuracy_score(test['home_win'], predict_test_data(test, X_train.columns, qda_model))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test Score: 0.625\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Test Score: {}\".format(\n",
    "    accuracy_score(test['home_win'], predict_test_data(test, X_train.columns, rf_model))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive! We now have an idea of what our more advanced model should hope to achieve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
