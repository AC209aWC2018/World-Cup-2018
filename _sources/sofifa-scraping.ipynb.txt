{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sofifa Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sofifa.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# referenced from https://realpython.com/python-web-scraping-practical-introduction/\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to scrape the content at 'url' by making a HTTP GET request. \n",
    "    If the content-type of the response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    from contextlib import closing\n",
    "    \n",
    "    try:\n",
    "        with closing(requests.get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "    except AssertionError as error:\n",
    "        print(error)\n",
    "        print('Error in scraping of url')\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if response is some kind of HTML/XML\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200\n",
    "           and content_type is not None\n",
    "           and content_type.find('html') > -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find all the href attributes for each national team\n",
    "\n",
    "def get_nation_href(date_url):\n",
    "    \"\"\"Given a date url corresponding to a specific update of FIFA 18, returns a dictionary \n",
    "    containing the teams and their hrefs\"\"\"\n",
    "    url = 'https://sofifa.com/teams/national'+date_url   \n",
    "    html = BeautifulSoup(simple_get(url), 'html.parser')\n",
    "\n",
    "    teams_href = {}\n",
    "\n",
    "    for link in html.find_all('a', attrs={'href': re.compile(\"^/team/.+\")}):\n",
    "        if link.get_text() not in teams_href:\n",
    "            teams_href[link.get_text()] = link.get('href')\n",
    "            \n",
    "    return teams_href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teams_href = get_nation_href('?v=WC18&e=159126&set=true') # corresponds to FIFA WC18 Expansion Jun 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the urls of all the national teams. We note that the current urls link to the most recent ratings of the teams. We can find the urls of all players in the national teams from this page as well. We start by collecting the urls of all players in the national teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_players_href(teams_href, date_url):\n",
    "    \"\"\"Given a dictionary of team hrefs and names and a segment of a date url, corresponding to a specific update of FIFA 18\n",
    "    returns a dictionary containing the original href, a list of all players in the team, and a list of all \n",
    "    players href in the team, for all teams\"\"\"\n",
    "    teams={}\n",
    "    for team, href in teams_href.items():\n",
    "        url_team = 'https://sofifa.com'+href+date_url \n",
    "        html_team = BeautifulSoup(simple_get(url_team), 'html.parser')\n",
    "\n",
    "        player_list = []\n",
    "        player_href_list = []\n",
    "        for link in html_team.find_all('a', attrs={'href': re.compile(\"^/player/.+\")}):\n",
    "            if link.get('href') not in player_href_list:\n",
    "                player_href_list.append(link.get('href'))\n",
    "            if link.get_text() not in player_list:\n",
    "                player_list.append(link.get_text())\n",
    "\n",
    "        teams[team] = {'href': href,\n",
    "                       'players': player_list,\n",
    "                       'players_href': player_href_list}\n",
    "    return teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teams = get_players_href(teams_href, '?v=WC18&e=159126&set=true') # corresponds to FIFA WC18 Expansion Jun 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the urls to all the players in the national teams, we can collect the individual player data. We will collect the key summarized data for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url_player = 'https://sofifa.com/player/231747/kylian-mbappe/'+'?v=18&e=159124&set=true'\n",
    "html_player = BeautifulSoup(simple_get(url_player), 'html.parser')\n",
    "club_nation = html_player.find_all('figure', attrs={'class': \"avatar avatar-sm\"})\n",
    "club_nation[0].find_next('a', attrs={'href': re.compile(\"^/team/.*\")}).find_next('span').get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# still does not sort teams and clubs correctly\n",
    "def get_players(teams, date_url):\n",
    "    \"\"\"Given the output dictionary from get_players_href,  \n",
    "    and a segment of a date url, corresponding to a specific update of FIFA 18,\n",
    "    returns a dataframe containing the summary data for each player in the teams\"\"\"\n",
    "    summary = []\n",
    "    for tname, _ in teams.items():\n",
    "        player_href = teams[tname]['players_href']\n",
    "        for i, player in enumerate(player_href):\n",
    "            url_player = 'https://sofifa.com'+player+date_url\n",
    "            html_player = BeautifulSoup(simple_get(url_player), 'html.parser')\n",
    "\n",
    "            h1 = html_player.find('h1')\n",
    "\n",
    "            name_id = h1.get_text()\n",
    "            name = name_id.split('(')[0]\n",
    "            ID = re.search(r'\\((.+)\\)', name_id).group(1).split(' ')[-1]\n",
    "            full_name = h1.find_next('div').next_element.strip()\n",
    "    \n",
    "            # account for multiple preferred positions\n",
    "            position = ''\n",
    "            for p in h1.find_next('div').find_all('span'):\n",
    "                pos = p.get_text()+' '\n",
    "                position += pos\n",
    "            position.strip()\n",
    "\n",
    "            age = re.search(r'Age (\\d+) .+' , h1.find_next('div').find_all('span')[-1].next_sibling).group(1)\n",
    "            \n",
    "            # account for +/- signs\n",
    "            span = html_player.find('div', attrs={'class': 'card-body stats'}).find_all('span')\n",
    "            span_label = (html_player\n",
    "                          .find('div', attrs={'class': 'card-body stats'})\n",
    "                          .find_all('span', attrs={'class': re.compile(\"label.+\")}))\n",
    "            \n",
    "            overall = float(span_label[0].get_text())\n",
    "            potential = float(span_label[1].get_text())\n",
    "            value = span[-2].get_text()\n",
    "            wage = span[-1].get_text()\n",
    "    \n",
    "            # account for different order of clubs and national teams\n",
    "            club_nation = html_player.find_all('figure', attrs={'class': \"avatar avatar-sm\"})\n",
    "            cn0 = club_nation[0].find_next('a', attrs={'href': re.compile(\"^/team/.*\")})\n",
    "            if len(club_nation) > 1:\n",
    "                cn1 = club_nation[1].find_next('a', attrs={'href': re.compile(\"^/team/.*\")})\n",
    "                if cn0.get_text() == tname:\n",
    "                    club = cn1.get_text()\n",
    "                    club_rating = float(cn1.find_next('span').get_text())\n",
    "                    team = cn0.get_text()\n",
    "                    team_rating = float(cn0.find_next('span').get_text())\n",
    "                else:\n",
    "                    club = cn0.get_text()\n",
    "                    club_rating = float(cn0.find_next('span').get_text())\n",
    "                    team = cn1.get_text()\n",
    "                    team_rating = float(cn1.find_next('span').get_text())\n",
    "            else:\n",
    "                if cn0.get_text() == tname:\n",
    "                    club = None\n",
    "                    club_rating = np.nan\n",
    "                    team = cn0.get_text()\n",
    "                    team_rating = float(cn0.find_next('span').get_text())\n",
    "                else:\n",
    "                    club = cn0.get_text()\n",
    "                    club_rating = float(cn0.find_next('span').get_text())\n",
    "                    team = tname\n",
    "                    team_rating = np.nan\n",
    "                \n",
    "            \n",
    "            summary.append([ID, name, full_name, position,\n",
    "                   age, overall, potential, value,\n",
    "                   wage, club, club_rating, team, team_rating])\n",
    "    \n",
    "    summary_dat = pd.DataFrame(summary, columns=['ID', 'name', 'full_name', 'position', \n",
    "                                                 'age', 'overall', 'potential', 'value',\n",
    "                                                 'wage', 'club', 'club_rating', 'team', 'team_rating'])\n",
    "    \n",
    "    return summary_dat\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## still does not sort teams and clubs correctly\n",
    "def get_players(teams, date_url):\n",
    "    \"\"\"Given the output dictionary from get_players_href,  \n",
    "    and a segment of a date url, corresponding to a specific update of FIFA 18,\n",
    "    returns a dataframe containing the summary data for each player in the teams\"\"\"\n",
    "    summary = []\n",
    "    for tname, _ in teams.items():\n",
    "        player_href = teams[tname]['players_href']\n",
    "        for i, player in enumerate(player_href):\n",
    "            url_player = 'https://sofifa.com'+player+date_url\n",
    "            html_player = BeautifulSoup(simple_get(url_player), 'html.parser')\n",
    "\n",
    "            h1 = html_player.find('h1')\n",
    "\n",
    "            name_id = h1.get_text()\n",
    "            name = name_id.split('(')[0]\n",
    "            ID = re.search(r'\\((.+)\\)', name_id).group(1).split(' ')[-1]\n",
    "            full_name = h1.find_next('div').next_element.strip()\n",
    "    \n",
    "            # account for multiple preferred positions\n",
    "            position = ''\n",
    "            for p in h1.find_next('div').find_all('span'):\n",
    "                pos = p.get_text()+' '\n",
    "                position += pos\n",
    "            position.strip()\n",
    "\n",
    "            age = re.search(r'Age (\\d+) .+' , h1.find_next('div').find_all('span')[-1].next_sibling).group(1)\n",
    "            \n",
    "            # account for +/- signs\n",
    "            span = html_player.find('div', attrs={'class': 'card-body stats'}).find_all('span')\n",
    "            span_label = (html_player\n",
    "                          .find('div', attrs={'class': 'card-body stats'})\n",
    "                          .find_all('span', attrs={'class': re.compile(\"label.+\")}))\n",
    "            \n",
    "            overall = float(span_label[0].get_text())\n",
    "            potential = float(span_label[1].get_text())\n",
    "            value = span[-2].get_text()\n",
    "            wage = span[-1].get_text()\n",
    "    \n",
    "            # account for different order of clubs and national teams\n",
    "            club_nation = html_player.find_all('a', attrs={'href': re.compile(\"^/team/.*\")})\n",
    "            if len(club_nation) > 1:\n",
    "                if club_nation[0].get_text() == tname:\n",
    "                    # account for players who are on loan. they will have 4 teams\n",
    "                    if len(club_nation) > 2:\n",
    "                        t = 0\n",
    "                        c = 2\n",
    "                    else:\n",
    "                        t = 0\n",
    "                        c = 1\n",
    "                else:\n",
    "                    # account for players who are on loan. they will have 4 teams\n",
    "                    if len(club_nation) > 2:\n",
    "                        t = 2\n",
    "                        c = 0\n",
    "                    else:\n",
    "                        t = 1\n",
    "                        c = 0\n",
    "                club = club_nation[c].get_text()\n",
    "                club_rating = float(club_nation[c].find_next('span').get_text())\n",
    "                team = club_nation[t].get_text()\n",
    "                team_rating = float(club_nation[t].find_next('span').get_text())\n",
    "            else:\n",
    "                if club_nation[0].get_text() == tname:\n",
    "                    club = None\n",
    "                    club_rating = np.nan\n",
    "                    team = club_nation[0].get_text()\n",
    "                    team_rating = float(club_nation[0].find_next('span').get_text())\n",
    "                else:\n",
    "                    club = club_nation[0].get_text()\n",
    "                    club_rating = float(club_nation[0].find_next('span').get_text())\n",
    "                    team = tname\n",
    "                    team_rating = np.nan # note that we can impute this later even though the webpage does not have this\n",
    "                         \n",
    "            summary.append([ID, name, full_name, position,\n",
    "                   age, overall, potential, value,\n",
    "                   wage, club, club_rating, team, team_rating])\n",
    "    \n",
    "    summary_dat = pd.DataFrame(summary, columns=['ID', 'name', 'full_name', 'position', \n",
    "                                                 'age', 'overall', 'potential', 'value',\n",
    "                                                 'wage', 'club', 'club_rating', 'team', 'team_rating'])\n",
    "    \n",
    "    return summary_dat\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for players in FIFA WC18 Expansion Jun 16, get their stats from FIFA 18 Jun 14\n",
    "players = get_players(teams, '?v=18&e=159124&set=true') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['212188' '176635' '188350' ..., '206566' '224769' '222402'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-e4543af2dc26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2680\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2682\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2683\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2724\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2725\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2726\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2727\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1325\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[1;32m-> 1327\u001b[1;33m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['212188' '176635' '188350' ..., '206566' '224769' '222402'] not in index\""
     ]
    }
   ],
   "source": [
    "players[players['ID']==204713]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players.to_csv('../datasets/sofifa/sofifa_players.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
