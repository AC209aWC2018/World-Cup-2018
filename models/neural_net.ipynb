{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Neural Network on Small Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of plain curiosity, we wondered how a simple neural network would perform on our problem (Everyone wants to try deep learning nowadays). Because our training set is so small, we do not believe that the neural network will outperform any of our models from the previous part. In fact, it might overfit to the training set and perform worse than our other models. This notebook is just to experiment around with neural networks and see its performance on a small dataset. We will only look at the outcome at 90 minutes and Softmax in the test set, since it would be pretty complicated to come up with a scheme for the WC Playoff Model configuration, especially since there are so few WC playoff matches in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/cleaned/train_final.csv')\n",
    "test = pd.read_csv('../data/cleaned/test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['overall_diff', 'attack_away_defence_home_diff', 'attack_diff',\n",
    "           'attack_home_defence_away_diff', 'defence_diff', 'midfield_diff',\n",
    "           'prestige_diff', 'growth_diff', 'full_age_diff',\n",
    "           'start_age_diff', 'value_euros_millions_diff',\n",
    "           'wage_euros_thousands_diff', 'goalkeeper_overall_diff',\n",
    "           'bup_dribbling_diff', 'bup_passing_diff', 'bup_speed_diff',\n",
    "           'cc_crossing_diff', 'cc_passing_diff', 'cc_shooting_diff',\n",
    "           'd_aggresion_diff', 'd_pressure_diff', 'd_width_diff', \n",
    "           'gdp_diff', 'is_home', 'raw_gdp_diff', \n",
    "           'win_momentum_past_1_games_diff', 'lose_momentum_past_1_games_diff',\n",
    "           'win_momentum_past_2_games_diff', 'lose_momentum_past_2_games_diff',\n",
    "           'win_momentum_past_3_games_diff', 'lose_momentum_past_3_games_diff',\n",
    "           'win_momentum_past_4_games_diff', 'lose_momentum_past_4_games_diff',\n",
    "           'win_momentum_past_5_games_diff', 'lose_momentum_past_5_games_diff',\n",
    "           \n",
    "]\n",
    "\n",
    "train = train[columns + ['home_win']]\n",
    "test = test[columns  + ['home_win', 'home_win_no_pk']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same seed as baseline to ensure same train and validation sets so that comparisons are valid.\n",
    "np.random.seed(14)\n",
    "X_train, X_valid = train_test_split(train, test_size = 0.2)\n",
    "y_train = X_train['home_win'].ravel()\n",
    "X_train = X_train.drop(['home_win'], axis = 1).values\n",
    "y_valid= X_valid['home_win'].ravel()\n",
    "X_valid = X_valid.drop(['home_win'], axis = 1).values\n",
    "y_test = test['home_win'].ravel()\n",
    "y_test_no_pk = test['home_win_no_pk'].ravel()\n",
    "X_test = test.drop(['home_win', 'home_win_no_pk'], axis = 1).values\n",
    "\n",
    "\n",
    "X_train_mean = X_train.mean(axis = 0)\n",
    "X_train_std = X_train.std(axis = 0)\n",
    "\n",
    "X_train = (X_train - X_train_mean) / (X_train_std)\n",
    "X_valid = (X_valid - X_train_mean) / (X_train_std)\n",
    "X_test = (X_test - X_train_mean) / (X_train_std)\n",
    "\n",
    "#keras.utils.to_categorical doesn't work with negative numbers\n",
    "y_train = y_train + 1\n",
    "y_valid = y_valid + 1\n",
    "y_test_no_pk = y_test + 1\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_valid = keras.utils.to_categorical(y_valid)\n",
    "y_test_no_pk = keras.utils.to_categorical(y_test_no_pk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simple neural network will have a total of 3 hidden node layers, with 15 nodes in each layer. We will add regularization on each layer and also add drop out layers to try to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 15)                540       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 48        \n",
      "=================================================================\n",
      "Total params: 1,068\n",
      "Trainable params: 1,068\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(15, activation=\"relu\", input_dim = X_train.shape[1], kernel_regularizer=regularizers.l2(1e-6)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(15, activation=\"relu\", kernel_regularizer = regularizers.l2(1e-6)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(15, activation=\"relu\", activity_regularizer = regularizers.l2(1e-6)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation=\"sigmoid\", kernel_regularizer = regularizers.l2(1e-6)))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x103368c18>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = Adam(amsgrad= True)\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer = adam, metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs = 200, batch_size = 64, validation_data = (X_valid, y_valid), verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Train Accuracy: 0.5570204350888611\n",
      "Neural Network Validation Accuracy: 0.5447368411641372\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural Network Train Accuracy: {}\".format(model.evaluate(X_train, y_train, verbose = 0)[1]))\n",
    "print(\"Neural Network Validation Accuracy: {}\".format(model.evaluate(X_valid, y_valid, verbose = 0)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Test Accuracy (90 Minutes): 0.625\n",
      "Neural Network Test Accuracy (Softmax): 0.625\n"
     ]
    }
   ],
   "source": [
    "y_pred_softmax = [np.where(np.argsort(val) == 1)[0][0]-1 if (i >= 48) & (np.argmax(val) == 1)\n",
    "                           else np.argmax(val)-1 for i, val in enumerate(model.predict(X_test))] \n",
    "\n",
    "print(\"Neural Network Test Accuracy (90 Minutes): {}\".format(model.evaluate(X_test, y_test_no_pk, verbose = 0)[1]))\n",
    "print(\"Neural Network Test Accuracy (Softmax): {}\".format(accuracy_score(y_test, y_pred_softmax)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisngly, the neural network actually did better than we thought it would. This most likely stems from the regularization we added as well as the dropout layers. However, the neural network did not perform as well as our best model.\n",
    "\n",
    "We have thus shown that neural networks do not really help improve accuracy in this small dataset; it shows that we do not really need that complex of a model in this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
