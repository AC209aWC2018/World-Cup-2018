{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first build some basic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from predict_test_data import predict_test_data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/cleaned/train_final.csv')\n",
    "test = pd.read_csv('../data/cleaned/test_final.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['game_date', 'home_team', 'away_team', 'home_score', 'away_score',\n",
       "       'tournament', 'country', 'neutral', 'overall_diff',\n",
       "       'attack_away_defence_home_diff', 'attack_diff',\n",
       "       'attack_home_defence_away_diff', 'defence_diff', 'midfield_diff',\n",
       "       'rank_diff', 'prestige_diff', 'growth_diff', 'full_age_diff',\n",
       "       'start_age_diff', 'value_euros_millions_diff',\n",
       "       'wage_euros_thousands_diff', 'goalkeeper_overall_diff',\n",
       "       'bup_dribbling_diff', 'bup_passing_diff', 'bup_speed_diff',\n",
       "       'cc_crossing_diff', 'cc_passing_diff', 'cc_shooting_diff',\n",
       "       'd_aggresion_diff', 'd_pressure_diff', 'd_width_diff', 'home_win',\n",
       "       'gdp_diff', 'is_home', 'raw_gdp_diff', 'score_past_3_games_diff',\n",
       "       'wins_past_5_games_diff', 'wins_home_against_away_3_games',\n",
       "       'score_past_4_games_diff', 'wins_home_against_away_1_games',\n",
       "       'score_past_5_games_diff', 'score_conceded_past_1_games_diff',\n",
       "       'wins_past_4_games_diff', 'wins_past_2_games_diff',\n",
       "       'wins_home_against_away_5_games', 'wins_past_1_games_diff',\n",
       "       'wins_home_against_away_4_games', 'score_past_1_games_diff',\n",
       "       'wins_past_3_games_diff', 'score_conceded_past_4_games_diff',\n",
       "       'score_conceded_past_5_games_diff', 'score_past_2_games_diff',\n",
       "       'wins_home_against_away_2_games', 'score_conceded_past_3_games_diff',\n",
       "       'score_conceded_past_2_games_diff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['game_date', 'home_team', 'away_team', 'home_score', 'away_score',\n",
       "       'tournament', 'country', 'neutral', 'overall_diff',\n",
       "       'attack_away_defence_home_diff', 'attack_diff',\n",
       "       'attack_home_defence_away_diff', 'defence_diff', 'midfield_diff',\n",
       "       'rank_diff', 'prestige_diff', 'growth_diff', 'full_age_diff',\n",
       "       'start_age_diff', 'value_euros_millions_diff',\n",
       "       'wage_euros_thousands_diff', 'goalkeeper_overall_diff',\n",
       "       'bup_dribbling_diff', 'bup_passing_diff', 'bup_speed_diff',\n",
       "       'cc_crossing_diff', 'cc_passing_diff', 'cc_shooting_diff',\n",
       "       'd_aggresion_diff', 'd_pressure_diff', 'd_width_diff', 'home_win',\n",
       "       'home_win_no_pk', 'Group', 'gdp_diff', 'is_home', 'raw_gdp_diff',\n",
       "       'score_past_3_games_diff', 'wins_past_5_games_diff',\n",
       "       'wins_home_against_away_3_games', 'score_past_4_games_diff',\n",
       "       'wins_home_against_away_1_games', 'score_past_5_games_diff',\n",
       "       'score_conceded_past_1_games_diff', 'wins_past_4_games_diff',\n",
       "       'wins_past_2_games_diff', 'wins_home_against_away_5_games',\n",
       "       'wins_past_1_games_diff', 'wins_home_against_away_4_games',\n",
       "       'score_past_1_games_diff', 'wins_past_3_games_diff',\n",
       "       'score_conceded_past_4_games_diff', 'score_conceded_past_5_games_diff',\n",
       "       'score_past_2_games_diff', 'wins_home_against_away_2_games',\n",
       "       'score_conceded_past_3_games_diff', 'score_conceded_past_2_games_diff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our most basic model would be to just predict the majority class every time. In this case, `home_win` = 1 is the majority class. What are is the training accuracy from just doing this \"prediction\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43806009488666314"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['home_win'].value_counts()[1] / len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty decent when we have 3 classes. What about the test accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.421875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test['home_win'], np.ones(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still decent. Any model we build should be better than this test accuracy of just guessing.\n",
    "\n",
    "Our baseline model will be pretty simple. We will utilize the differences in FIFA rankings, offense ratings, defense ratings, midfield ratings, overall ratings, and whether the home team is actually playing at home. We will make a train and validation set out of the original train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['home_win', 'rank_diff', 'attack_diff', 'defence_diff', 'midfield_diff', 'overall_diff']]\n",
    "test = test[['home_win', 'rank_diff', 'attack_diff', 'defence_diff', 'midfield_diff', 'overall_diff', 'Group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "X_train, X_validation = train_test_split(train, test_size = 0.2)\n",
    "y_train = X_train['home_win'].ravel()\n",
    "X_train = X_train.drop(['home_win'], axis = 1)\n",
    "y_validation = X_validation['home_win'].ravel()\n",
    "X_validation = X_validation.drop(['home_win'], axis = 1)\n",
    "y_test = test['home_win'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the score of each model\n",
    "score = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try out logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegressionCV(solver = 'lbfgs', max_iter = 5000, cv = 5, multi_class='multinomial').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Score: 0.5102175346077785\n",
      "Logistic Regression Validation Score: 0.5710526315789474\n"
     ]
    }
   ],
   "source": [
    "score[\"Logistic Regression\"] = {}\n",
    "score[\"Logistic Regression\"][\"model\"] = lr_model\n",
    "score[\"Logistic Regression\"][\"Train Score\"] = lr_model.score(X_train, y_train)\n",
    "score[\"Logistic Regression\"][\"Validation Score\"] = lr_model.score(X_validation, y_validation)\n",
    "\n",
    "print(\"Logistic Regression Train Score: {}\".format(score[\"Logistic Regression\"][\"Train Score\"]))\n",
    "print(\"Logistic Regression Validation Score: {}\".format(score[\"Logistic Regression\"][\"Validation Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also try out Linear Discriminant Analysis. However, we need to first check whether the variances across the three outcomes are equal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_diff</th>\n",
       "      <th>attack_diff</th>\n",
       "      <th>defence_diff</th>\n",
       "      <th>midfield_diff</th>\n",
       "      <th>overall_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_win</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>916.793268</td>\n",
       "      <td>40.858572</td>\n",
       "      <td>37.554931</td>\n",
       "      <td>38.133908</td>\n",
       "      <td>31.393396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>960.540851</td>\n",
       "      <td>47.654088</td>\n",
       "      <td>43.559341</td>\n",
       "      <td>41.798216</td>\n",
       "      <td>36.158215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>936.866455</td>\n",
       "      <td>49.295727</td>\n",
       "      <td>43.509512</td>\n",
       "      <td>42.634416</td>\n",
       "      <td>37.105145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rank_diff  attack_diff  defence_diff  midfield_diff  overall_diff\n",
       "home_win                                                                    \n",
       "-1        916.793268    40.858572     37.554931      38.133908     31.393396\n",
       " 0        960.540851    47.654088     43.559341      41.798216     36.158215\n",
       " 1        936.866455    49.295727     43.509512      42.634416     37.105145"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('home_win').var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, besides `rank_diff`, they are actually quite similar! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LinearDiscriminantAnalysis().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Train Score: 0.5042847725774555\n",
      "LDA Validation Score: 0.5710526315789474\n"
     ]
    }
   ],
   "source": [
    "score[\"LDA\"] = {}\n",
    "score[\"LDA\"][\"model\"] = lda_model\n",
    "score[\"LDA\"][\"Train Score\"] = lda_model.score(X_train, y_train)\n",
    "score[\"LDA\"][\"Validation Score\"] = lda_model.score(X_validation, y_validation)\n",
    "print(\"LDA Train Score: {}\".format(score[\"LDA\"][\"Train Score\"]))\n",
    "print(\"LDA Validation Score: {}\".format(score[\"LDA\"][\"Validation Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also try out Quadratic Discriminant Analysis, which should perform similarly to LDA in this case due to the almost equal variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_model = QuadraticDiscriminantAnalysis().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA Train Score: 0.5049439683586026\n",
      "QDA Validation Score: 0.5552631578947368\n"
     ]
    }
   ],
   "source": [
    "score[\"QDA\"] = {}\n",
    "score[\"QDA\"]['model'] = qda_model\n",
    "score[\"QDA\"][\"Train Score\"] = qda_model.score(X_train, y_train)\n",
    "score[\"QDA\"][\"Validation Score\"] = qda_model.score(X_validation, y_validation)\n",
    "print(\"QDA Train Score: {}\".format(score[\"QDA\"][\"Train Score\"]))\n",
    "print(\"QDA Validation Score: {}\".format(score[\"QDA\"][\"Validation Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also try out Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:   36.1s finished\n"
     ]
    }
   ],
   "source": [
    "rf_params = {'bootstrap': [True, False],\n",
    "     'max_depth': [3, 5, 10, 20, 30, 40, None],\n",
    "'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4, 10, 20],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [10, 50, 100, 200, 500]}\n",
    "\n",
    "rf_model = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions=rf_params,\\\n",
    "                                   n_iter=50, scoring='accuracy', n_jobs=-1, cv=5, verbose=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Score: 0.5154911008569545\n",
      "Random Forest Validation Score 0.5657894736842105\n"
     ]
    }
   ],
   "source": [
    "score[\"Random Forest\"] = {}\n",
    "score[\"Random Forest\"]['model'] = rf_model\n",
    "score[\"Random Forest\"][\"Train Score\"] = rf_model.score(X_train, y_train)\n",
    "score[\"Random Forest\"][\"Validation Score\"] = rf_model.score(X_validation, y_validation)\n",
    "print(\"Random Forest Train Score: {}\".format(score[\"Random Forest\"][\"Train Score\"]))\n",
    "print(\"Random Forest Validation Score {}\".format(score[\"Random Forest\"][\"Validation Score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also not forget XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:   18.6s finished\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [3, 4, 5]\n",
    "    }\n",
    "xgb_model = RandomizedSearchCV(estimator=XGBClassifier(objective='multi:softmax', num_class = 3), param_distributions=xgb_params,\\\n",
    "                                   n_iter=50, scoring='accuracy', n_jobs=-1, cv=5, verbose=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Train Score: 0.5154911008569545\n",
      "XGBoost Validation Score 0.5657894736842105\n"
     ]
    }
   ],
   "source": [
    "score[\"XGBoost\"] = {}\n",
    "score[\"XGBoost\"]['model'] = xgb_model\n",
    "score[\"XGBoost\"][\"Train Score\"] = xgb_model.score(X_train, y_train)\n",
    "score[\"XGBoost\"][\"Validation Score\"] = xgb_model.score(X_validation, y_validation)\n",
    "print(\"XGBoost Train Score: {}\".format(score[\"Random Forest\"][\"Train Score\"]))\n",
    "print(\"XGBoost Validation Score {}\".format(score[\"Random Forest\"][\"Validation Score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(score).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Validation Score</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.510218</td>\n",
       "      <td>0.571053</td>\n",
       "      <td>LogisticRegressionCV(Cs=10, class_weight=None,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.504285</td>\n",
       "      <td>0.571053</td>\n",
       "      <td>LinearDiscriminantAnalysis(n_components=None, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.504944</td>\n",
       "      <td>0.555263</td>\n",
       "      <td>QuadraticDiscriminantAnalysis(priors=None, reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.515491</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>RandomizedSearchCV(cv=5, error_score='raise-de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.55</td>\n",
       "      <td>RandomizedSearchCV(cv=5, error_score='raise-de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Train Score Validation Score  \\\n",
       "Logistic Regression    0.510218         0.571053   \n",
       "LDA                    0.504285         0.571053   \n",
       "QDA                    0.504944         0.555263   \n",
       "Random Forest          0.515491         0.565789   \n",
       "XGBoost                0.512195             0.55   \n",
       "\n",
       "                                                                 model  \n",
       "Logistic Regression  LogisticRegressionCV(Cs=10, class_weight=None,...  \n",
       "LDA                  LinearDiscriminantAnalysis(n_components=None, ...  \n",
       "QDA                  QuadraticDiscriminantAnalysis(priors=None, reg...  \n",
       "Random Forest        RandomizedSearchCV(cv=5, error_score='raise-de...  \n",
       "XGBoost              RandomizedSearchCV(cv=5, error_score='raise-de...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We choose the final model to be the one with the highest validation score, which is Logistic Regression in this case\n"
     ]
    }
   ],
   "source": [
    "model_name = df_result['Validation Score'].astype(float).argmax()\n",
    "print(\"We choose the final model to be the one with the highest validation score,\\\n",
    " which is {} in this case\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the best Model, Logistic Regression, the test accuracy is 0.609\n"
     ]
    }
   ],
   "source": [
    "test_pred = predict_test_data(test, X_train.columns, df_result.loc[model_name].model)\n",
    "test_score = accuracy_score(y_test, test_pred)\n",
    "print(\"For the best Model, {}, the test accuracy is {:.3f}\".format(model_name, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive! We now have an idea of what our more advanced model should hope to achieve. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
